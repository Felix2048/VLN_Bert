{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "WARNING:tensorflow:From /home/mikel/miniconda3/envs/vilbert-mt/lib/python3.6/site-packages/tensorpack/callbacks/hooks.py:17: The name tf.train.SessionRunHook is deprecated. Please use tf.estimator.SessionRunHook instead.\n\nWARNING:tensorflow:From /home/mikel/miniconda3/envs/vilbert-mt/lib/python3.6/site-packages/tensorpack/tfutils/optimizer.py:18: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n\nWARNING:tensorflow:From /home/mikel/miniconda3/envs/vilbert-mt/lib/python3.6/site-packages/tensorpack/tfutils/sesscreate.py:20: The name tf.train.SessionCreator is deprecated. Please use tf.compat.v1.train.SessionCreator instead.\n\n"
    }
   ],
   "source": [
    "import logging\n",
    "import json\n",
    "import torch\n",
    "from types import SimpleNamespace\n",
    "from vilbert.vilbert import VILBertActionGrounding, BertConfig, VILBertActionSelection\n",
    "from pytorch_transformers.tokenization_bert import BertTokenizer\n",
    "from pytorch_transformers.optimization import AdamW, WarmupLinearSchedule\n",
    "import torch.distributed as dist\n",
    "from VLN_config import config as args\n",
    "import random\n",
    "import pandas as pd\n",
    "from data.dataLoaderActSelection import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import argparse\n",
    "import glob\n",
    "import pdb\n",
    "\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from faster_rcnn import feature_extractor_new as f_extractor\n",
    "from faster_rcnn.feature_extractor_new import featureExtractor, visualize_tensor\n",
    "#%matplotlib inline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "namespace(adam_epsilon=1e-08, baseline=False, bert_model='bert-base-uncased', best_features=5, clean_train_sets=True, clip_size=3, config_file='config/bert_base_6layer_6conect.json', distributed=False, do_lower_case=True, dynamic_attention=False, epochs=100, from_pretrained='save/action_grounding/best_val.bin', gradient_accumulation_steps=1, img_weight=1, in_memory=False, learning_rate=0.0001, local_rank=-1, max_temporal_memory_buffer=3, mean_layer=False, num_key_frames=2, num_train_epochs=10.0, num_workers=0, objective=1, predict_feature=False, save_name='', seed=42, split='mteval', start_epoch=0, task_specific_tokens=True, tasks='1', threshold_similarity=0.7, track_temporal_features=True, train_batch_size=4, use_tensorboard=True, visual_target=0, warmup_proportion=0.1, without_coattention=False)\n"
    }
   ],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "08/03/2020 13:13:16 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/mikel/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n"
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    args.bert_model, do_lower_case=args.do_lower_case\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "08/03/2020 13:13:17 - INFO - pytorch_transformers.tokenization_utils -   loading file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased-vocab.txt from cache at /home/mikel/.cache/torch/pytorch_transformers/26bc1ad6c0ac742e9b52263248f6d0f00068293b33709fae12320c0e35ccfbbb.542ce4285a40d23a559526243235df47c5f75c197f04f37d1a0c124c32c9a084\n08/03/2020 13:13:17 - INFO - vilbert.utils -   loading weights file ./save/action_selection/best_train_vilberActionSelection.bin\n./save/action_selection/best_train_vilberActionSelection.bin\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "tLayer(\n          (attention): BertAttention(\n            (self): BertSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (v_layer): ModuleList(\n        (0): BertImageLayer(\n          (attention): BertImageAttention(\n            (self): BertImageSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertImageSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertImageIntermediate(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (output): BertImageOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertImageLayer(\n          (attention): BertImageAttention(\n            (self): BertImageSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertImageSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertImageIntermediate(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (output): BertImageOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertImageLayer(\n          (attention): BertImageAttention(\n            (self): BertImageSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertImageSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertImageIntermediate(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (output): BertImageOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertImageLayer(\n          (attention): BertImageAttention(\n            (self): BertImageSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertImageSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertImageIntermediate(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (output): BertImageOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertImageLayer(\n          (attention): BertImageAttention(\n            (self): BertImageSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertImageSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertImageIntermediate(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (output): BertImageOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertImageLayer(\n          (attention): BertImageAttention(\n            (self): BertImageSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): BertImageSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): BertLayerNorm()\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): BertImageIntermediate(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (output): BertImageOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n      (c_layer): ModuleList(\n        (0): BertConnectionLayer(\n          (biattention): BertBiAttention(\n            (query1): Linear(in_features=1024, out_features=1024, bias=True)\n            (key1): Linear(in_features=1024, out_features=1024, bias=True)\n            (value1): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout1): Dropout(p=0.1, inplace=False)\n            (query2): Linear(in_features=768, out_features=1024, bias=True)\n            (key2): Linear(in_features=768, out_features=1024, bias=True)\n            (value2): Linear(in_features=768, out_features=1024, bias=True)\n            (dropout2): Dropout(p=0.1, inplace=False)\n          )\n          (biOutput): BertBiOutput(\n            (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm1): BertLayerNorm()\n            (dropout1): Dropout(p=0.1, inplace=False)\n            (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_dropout1): Dropout(p=0.1, inplace=False)\n            (dense2): Linear(in_features=1024, out_features=768, bias=True)\n            (LayerNorm2): BertLayerNorm()\n            (dropout2): Dropout(p=0.1, inplace=False)\n            (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n            (q_dropout2): Dropout(p=0.1, inplace=False)\n          )\n          (v_intermediate): BertImageIntermediate(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (v_output): BertImageOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (t_intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (t_output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): BertConnectionLayer(\n          (biattention): BertBiAttention(\n            (query1): Linear(in_features=1024, out_features=1024, bias=True)\n            (key1): Linear(in_features=1024, out_features=1024, bias=True)\n            (value1): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout1): Dropout(p=0.1, inplace=False)\n            (query2): Linear(in_features=768, out_features=1024, bias=True)\n            (key2): Linear(in_features=768, out_features=1024, bias=True)\n            (value2): Linear(in_features=768, out_features=1024, bias=True)\n            (dropout2): Dropout(p=0.1, inplace=False)\n          )\n          (biOutput): BertBiOutput(\n            (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm1): BertLayerNorm()\n            (dropout1): Dropout(p=0.1, inplace=False)\n            (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_dropout1): Dropout(p=0.1, inplace=False)\n            (dense2): Linear(in_features=1024, out_features=768, bias=True)\n            (LayerNorm2): BertLayerNorm()\n            (dropout2): Dropout(p=0.1, inplace=False)\n            (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n            (q_dropout2): Dropout(p=0.1, inplace=False)\n          )\n          (v_intermediate): BertImageIntermediate(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (v_output): BertImageOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (t_intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (t_output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): BertConnectionLayer(\n          (biattention): BertBiAttention(\n            (query1): Linear(in_features=1024, out_features=1024, bias=True)\n            (key1): Linear(in_features=1024, out_features=1024, bias=True)\n            (value1): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout1): Dropout(p=0.1, inplace=False)\n            (query2): Linear(in_features=768, out_features=1024, bias=True)\n            (key2): Linear(in_features=768, out_features=1024, bias=True)\n            (value2): Linear(in_features=768, out_features=1024, bias=True)\n            (dropout2): Dropout(p=0.1, inplace=False)\n          )\n          (biOutput): BertBiOutput(\n            (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm1): BertLayerNorm()\n            (dropout1): Dropout(p=0.1, inplace=False)\n            (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_dropout1): Dropout(p=0.1, inplace=False)\n            (dense2): Linear(in_features=1024, out_features=768, bias=True)\n            (LayerNorm2): BertLayerNorm()\n            (dropout2): Dropout(p=0.1, inplace=False)\n            (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n            (q_dropout2): Dropout(p=0.1, inplace=False)\n          )\n          (v_intermediate): BertImageIntermediate(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (v_output): BertImageOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (t_intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (t_output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): BertConnectionLayer(\n          (biattention): BertBiAttention(\n            (query1): Linear(in_features=1024, out_features=1024, bias=True)\n            (key1): Linear(in_features=1024, out_features=1024, bias=True)\n            (value1): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout1): Dropout(p=0.1, inplace=False)\n            (query2): Linear(in_features=768, out_features=1024, bias=True)\n            (key2): Linear(in_features=768, out_features=1024, bias=True)\n            (value2): Linear(in_features=768, out_features=1024, bias=True)\n            (dropout2): Dropout(p=0.1, inplace=False)\n          )\n          (biOutput): BertBiOutput(\n            (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm1): BertLayerNorm()\n            (dropout1): Dropout(p=0.1, inplace=False)\n            (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_dropout1): Dropout(p=0.1, inplace=False)\n            (dense2): Linear(in_features=1024, out_features=768, bias=True)\n            (LayerNorm2): BertLayerNorm()\n            (dropout2): Dropout(p=0.1, inplace=False)\n            (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n            (q_dropout2): Dropout(p=0.1, inplace=False)\n          )\n          (v_intermediate): BertImageIntermediate(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (v_output): BertImageOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (t_intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (t_output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): BertConnectionLayer(\n          (biattention): BertBiAttention(\n            (query1): Linear(in_features=1024, out_features=1024, bias=True)\n            (key1): Linear(in_features=1024, out_features=1024, bias=True)\n            (value1): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout1): Dropout(p=0.1, inplace=False)\n            (query2): Linear(in_features=768, out_features=1024, bias=True)\n            (key2): Linear(in_features=768, out_features=1024, bias=True)\n            (value2): Linear(in_features=768, out_features=1024, bias=True)\n            (dropout2): Dropout(p=0.1, inplace=False)\n          )\n          (biOutput): BertBiOutput(\n            (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm1): BertLayerNorm()\n            (dropout1): Dropout(p=0.1, inplace=False)\n            (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_dropout1): Dropout(p=0.1, inplace=False)\n            (dense2): Linear(in_features=1024, out_features=768, bias=True)\n            (LayerNorm2): BertLayerNorm()\n            (dropout2): Dropout(p=0.1, inplace=False)\n            (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n            (q_dropout2): Dropout(p=0.1, inplace=False)\n          )\n          (v_intermediate): BertImageIntermediate(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (v_output): BertImageOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (t_intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (t_output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): BertConnectionLayer(\n          (biattention): BertBiAttention(\n            (query1): Linear(in_features=1024, out_features=1024, bias=True)\n            (key1): Linear(in_features=1024, out_features=1024, bias=True)\n            (value1): Linear(in_features=1024, out_features=1024, bias=True)\n            (dropout1): Dropout(p=0.1, inplace=False)\n            (query2): Linear(in_features=768, out_features=1024, bias=True)\n            (key2): Linear(in_features=768, out_features=1024, bias=True)\n            (value2): Linear(in_features=768, out_features=1024, bias=True)\n            (dropout2): Dropout(p=0.1, inplace=False)\n          )\n          (biOutput): BertBiOutput(\n            (dense1): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm1): BertLayerNorm()\n            (dropout1): Dropout(p=0.1, inplace=False)\n            (q_dense1): Linear(in_features=1024, out_features=1024, bias=True)\n            (q_dropout1): Dropout(p=0.1, inplace=False)\n            (dense2): Linear(in_features=1024, out_features=768, bias=True)\n            (LayerNorm2): BertLayerNorm()\n            (dropout2): Dropout(p=0.1, inplace=False)\n            (q_dense2): Linear(in_features=1024, out_features=768, bias=True)\n            (q_dropout2): Dropout(p=0.1, inplace=False)\n          )\n          (v_intermediate): BertImageIntermediate(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n          )\n          (v_output): BertImageOutput(\n            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n          (t_intermediate): BertIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (t_output): BertOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): BertLayerNorm()\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n    (t_pooler): BertTextPooler(\n      (dense): Linear(in_features=768, out_features=1024, bias=True)\n      (activation): ReLU()\n    )\n    (v_pooler): BertImagePooler(\n      (dense): Linear(in_features=1024, out_features=1024, bias=True)\n      (activation): ReLU()\n    )\n  )\n  (dropout): Dropout(p=0.1, inplace=False)\n  (vil_prediction_action_selection): SimpleClassifier(\n    (logit_fc): Sequential(\n      (0): Linear(in_features=1024, out_features=2048, bias=True)\n      (1): GeLU()\n      (2): BertLayerNorm()\n      (3): Linear(in_features=2048, out_features=13, bias=True)\n    )\n  )\n)"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "config = BertConfig.from_json_file(args.config_file)\n",
    "bert_weight_name = json.load(\n",
    "    open(\"config/\" + args.bert_model + \"_weight_name.json\", \"r\")\n",
    ")\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\n",
    "    args.bert_model, do_lower_case=args.do_lower_case\n",
    ")\n",
    "\n",
    "config.track_temporal_features = args.track_temporal_features\n",
    "config.mean_layer = args.mean_layer\n",
    "config.max_temporal_memory_buffer = args.max_temporal_memory_buffer\n",
    "config.visualization = True\n",
    "\n",
    "#The path of the finetuned ActionGrounding weights\n",
    "args.from_pretrained = \"./save/action_selection/best_train_vilberActionSelection.bin\"\n",
    "\n",
    "print(args.from_pretrained)\n",
    "model = VILBertActionSelection.from_pretrained(\n",
    "    args.from_pretrained, config=config, default_gpu=True\n",
    ")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "frcnn_model = models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "data_loader = DataLoader(\"data/json_data.json\", frcnn_model, save_or_not = False)\n",
    "path = 'data/DataLoaderActSelection.pt'\n",
    "data_loaded = data_loader.load_dataloader(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 12, got 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-e6745702895a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfeatures_masked\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_enc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspatial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_lm_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mco_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_img_labels\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mdata_loaded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 12, got 10)"
     ]
    }
   ],
   "source": [
    "features_masked, pos_enc, spatial, image_mask, tokenized_text, masked_text, masked_lm_token, input_mask, segment_ids, co_attention_mask, infos, masked_img_labels  = data_loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, pos_enc, spatial, image_mask, tokenized_text, input_mask, segment_ids, co_attention_mask, infos, action_targets = data_loaded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Forward the model with one input example\n",
    "model.eval()\n",
    "pred_action_train, att_train = model(input_ids = tokenized_text[:5].cpu(),\n",
    "                                image_feat = features[:5].cpu(), # Linear(2048*config.max_temporal_memory_buffer, 2048)\n",
    "                                image_loc = spatial[:5].cpu(),  #Linear(in_features=5, out_features=1024, bias=True)\n",
    "                                image_pos_input = pos_enc[:5].cpu(),   #Linear(7, 2048)/(6, 2048)\n",
    "                                token_type_ids = segment_ids[:5].cpu(), \n",
    "                                attention_mask = input_mask[:5].cpu(), \n",
    "                                image_attention_mask = image_mask[:5].cpu(),\n",
    "                                output_all_attention_masks=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action prediction and attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "predicted token 4\ngt token :  4\nPrediction:  LookDown\nGT:  LookDown\nThe accuracy over train data set:  40.0\n"
    }
   ],
   "source": [
    "data_loader = DataLoader(\"data/json_data.json\", frcnn_model, save_or_not = False)\n",
    "i = 0\n",
    "id_action_masked = torch.argmax(pred_action_train[i]).item()\n",
    "print(\"predicted token\" , id_action_masked)\n",
    "print(\"gt token : \", action_targets[i].item())\n",
    "predicted_masked_action = data_loader.id_to_action(id_action_masked)\n",
    "print(\"Prediction: \", predicted_masked_action)\n",
    "print(\"GT: \",data_loader.id_to_action(action_targets[i].item()))\n",
    "from sklearn.metrics import accuracy_score as accuracy\n",
    "acc = accuracy(action_targets[:5].numpy(), torch.argmax(pred_action_train, dim=1).numpy())\n",
    "print(\"The accuracy over train data set: \", acc*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "require.config({\n",
    "  paths: {\n",
    "      d3: '//cdnjs.cloudflare.com/ajax/libs/d3/3.4.8/d3.min',\n",
    "      jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min',\n",
    "  }\n",
    "});"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def format_attention(attention):\n",
    "    squeezed = []\n",
    "    for layer_attention in attention:\n",
    "        # 1 x num_heads x seq_len x seq_len\n",
    "        if len(layer_attention.shape) != 4:\n",
    "            raise ValueError(\"The attention tensor does not have the correct number of dimensions. Make sure you set \"\n",
    "                             \"output_attentions=True when initializing your model.\")\n",
    "        squeezed.append(layer_attention.squeeze(0))\n",
    "    # num_layers x num_heads x seq_len x seq_len\n",
    "    return torch.stack(squeezed)\n",
    "\n",
    "def format_special_chars(tokens):\n",
    "    return [t.replace('Ġ', ' ').replace('▁', ' ').replace('</w>', '') for t in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from IPython.core.display import display, HTML, Javascript\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "def head_view(attention, tokens, tokens_l, sentence_b_start = None, prettify_tokens=False):\n",
    "    \"\"\"Render head view\n",
    "        Args:\n",
    "            attention: list of ``torch.FloatTensor``(one for each layer) of shape\n",
    "                ``(batch_size(must be 1), num_heads, sequence_length, sequence_length)``\n",
    "            tokens: list of tokens\n",
    "            sentence_b_index: index of first wordpiece in sentence B if input text is sentence pair (optional)\n",
    "            prettify_tokens: indicates whether to remove special characters in wordpieces, e.g. Ġ\n",
    "    \"\"\"\n",
    "\n",
    "    if sentence_b_start is not None:\n",
    "        vis_html = \"\"\"\n",
    "        <span style=\"user-select:none\">\n",
    "            Layer: <select id=\"layer\"></select>\n",
    "            Attention: <select id=\"filter\">\n",
    "              <option value=\"all\">All</option>\n",
    "              <option value=\"aa\">Sentence A -> Sentence A</option>\n",
    "              <option value=\"ab\">Sentence A -> Sentence B</option>\n",
    "              <option value=\"ba\">Sentence B -> Sentence A</option>\n",
    "              <option value=\"bb\">Sentence B -> Sentence B</option>\n",
    "            </select>\n",
    "            </span>\n",
    "        <div id='vis'></div>\n",
    "        \"\"\"\n",
    "    else:\n",
    "        vis_html = \"\"\"\n",
    "              <span style=\"user-select:none\">\n",
    "                Layer: <select id=\"layer\"></select>\n",
    "              </span>\n",
    "              <div id='vis'></div> \n",
    "            \"\"\"\n",
    "\n",
    "    display(HTML(vis_html))\n",
    "    __location__ = os.path.realpath(\n",
    "        os.path.join(os.getcwd()))#, os.path.dirname(__file__)))\n",
    "    vis_js = open(os.path.join(__location__, 'head_view.js')).read()\n",
    "\n",
    "    if prettify_tokens:\n",
    "        tokens = format_special_chars(tokens)\n",
    "\n",
    "    attn = format_attention(attention)\n",
    "    attn_data = {\n",
    "        'all': {\n",
    "            'attn': attn.tolist(),\n",
    "            'left_text': tokens_l,\n",
    "            'right_text': tokens\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    params = {\n",
    "        'attention': attn_data,\n",
    "        'default_filter': \"all\"\n",
    "    }\n",
    "    attn_seq_len = len(attn_data['all']['attn'][0][0])\n",
    "    print(attn_seq_len)\n",
    "    if attn_seq_len != len(tokens):\n",
    "        pass\n",
    "        #raise ValueError(f\"Attention has {attn_seq_len} positions, while number of tokens is {len(tokens)}\")\n",
    "\n",
    "    display(Javascript('window.params = %s' % json.dumps(params)))\n",
    "    display(Javascript(vis_js))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = [\n",
    "    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
    "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A', 'stop sign',\n",
    "    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n",
    "    'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack', 'umbrella', 'N/A', 'N/A',\n",
    "    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n",
    "    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n",
    "    'bottle', 'N/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n",
    "    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n",
    "    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table',\n",
    "    'N/A', 'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n",
    "    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A', 'book',\n",
    "    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (all_attention_mask_t, all_attnetion_mask_v, all_attention_mask_c) = att_train\n",
    "def preprocess_att_viz(data_idx, att_idx):\n",
    "    global att_train, infos, classes\n",
    "    att_list = []\n",
    "    for i in range(len(att_train[att_idx])):\n",
    "        if att_idx == 2:\n",
    "            att_list.append(att_train[att_idx][i][\"attn2\"][data_idx].unsqueeze(0))\n",
    "        else:\n",
    "            att_list.append(att_train[att_idx][i][\"attn\"][data_idx].unsqueeze(0))\n",
    "    input_id_list = masked_text[data_idx].tolist()\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_id_list) \n",
    "#     img_reg = [1]*att_list[att_idx].shape[2] + [0]*att_list[att_idx].shape[-1]\n",
    "    img_reg = [classes[info] for info in infos[0][0][\"objects\"].tolist()] # + ['PAD']*abs(att_list[att_idx].shape[-1] - infos[data_idx][\"objects\"].shape[0])  \n",
    "    if att_idx==0:\n",
    "        return att_list, tokens, tokens\n",
    "    if att_idx==1:\n",
    "        return att_list, img_reg, img_reg\n",
    "    else:\n",
    "        return att_list, tokens, img_reg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_list, token1, token2= preprocess_att_viz(2, 0)\n",
    "head_view(att_list, token1, token1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image prediction and attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=5\n",
    "#print(tokenizer.convert_ids_to_tokens(masked_text[i].tolist()))\n",
    "#To set which word is masked\n",
    "for j in range(len(masked_img_labels[i])):\n",
    "    if masked_img_labels[i][j].item()!=-1:\n",
    "        break\n",
    "id_word_masked = torch.argmax(pred_v_train[i,j,:]).item()\n",
    "print(\"predicted token\" , id_word_masked)\n",
    "print(\"gt token : \", masked_img_labels[i][j].item())\n",
    "print(\"Prediction: \", classes[id_word_masked])\n",
    "print(\"GT: \", classes[masked_img_labels[i][j].item()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_list, token1, token2= preprocess_att_viz(0, 1)\n",
    "head_view(att_list, token2, token2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image and Text prediction co-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "att_list, token1, token2= preprocess_att_viz(0, 2)\n",
    "head_view(att_list, token1, token2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(infos[0][0])\n",
    "visualize_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.10 64-bit ('vilbert-mt': conda)",
   "language": "python",
   "name": "python361064bitvilbertmtconda7bb1bd3e955c469187a4ae8b5b7d6907"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}